{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eli5\n",
    "import graphviz\n",
    "import shap\n",
    "shap.initjs()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp#, get_dataset, info_plots\n",
    "\n",
    "from visualization_utils import load_notebook_config, show_feature_importance\n",
    "load_notebook_config(static=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Motivation\n",
    "\n",
    "- Permutation importance\n",
    "\n",
    "- Partial Dependence Plots\n",
    "\n",
    "- SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "\n",
    "Explore techniques to extract the following insights from machine learning models:\n",
    "\n",
    "- What features in the data did the model think are most important?\n",
    "\n",
    "- For any single prediction from a model, how did each feature in the data affect that particular prediction?\n",
    "\n",
    "- How does each feature affect the model's predictions in a big-picture sense (what is its typical effect when considered over a large number of possible predictions)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "\n",
    "Why Are These Insights Valuable?\n",
    "\n",
    "- Debugging\n",
    "\n",
    "- Informing feature engineering\n",
    "\n",
    "- Directing future data collection\n",
    "\n",
    "- Informing human decision-making\n",
    "\n",
    "- Building Trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What features have the biggest impact on predictions?\n",
    "\n",
    "- Only gives you notion which features contributes to the decision, not \"which way\".\n",
    "\n",
    "- Permutation importance: a feature importance technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fast to calculate\n",
    "\n",
    "- Widely used and understood\n",
    "\n",
    "- It is calculated after a model has been fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If I randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data?\n",
    "\n",
    "\n",
    "- Example: \"We want to predict a person's height when they become 20 years old, using data that is available at age 10.\"\n",
    "\n",
    "<img src=\"../images/permutation_importance_example_1.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The process\n",
    "\n",
    "\n",
    "1. Get a trained model.\n",
    "\n",
    "2. Shuffle the values in a single column, make predictions using the resulting dataset. Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. That performance deterioration measures the importance of the variable you just shuffled.\n",
    "\n",
    "3. Return the data to the original order (undoing the shuffle from step 2). Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code example with eli5 library\n",
    "\n",
    "\n",
    "- The idea is to use a model that predicts whether a football team will have the \"Man of the Game\" winner based on the team's statistics.\n",
    "\n",
    "- https://www.kaggle.com/mathan/fifa-2018-match-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fifa2018 = pd.read_csv('../data/FIFA_2018_Statistics.csv')\n",
    "print(fifa2018.shape)\n",
    "fifa2018.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code example with eli5 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = (fifa2018['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\n",
    "feature_names = [i for i in fifa2018.columns if fifa2018[i].dtype in [np.int64]]\n",
    "X = fifa2018[feature_names]\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "my_model = RandomForestClassifier(n_estimators=100, random_state=0).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The first number in each row shows how much model performance decreased with a random shuffling. The number after the ± measures how performance varied from one-reshuffling to the next.\n",
    "\n",
    "- You'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Warnings\n",
    "\n",
    "\n",
    "- Codependent features tend to share importance.\n",
    "\n",
    "- This approach is faster but can introduce nonsensical observations by permuting invalid values into records (e.g., shifting a true pregnant value into a male’s record).\n",
    "\n",
    "\n",
    "(Warnings source: Martin's feature importance docs :D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- While feature importance shows WHAT VARIABLES most affect predictions, partial dependence plots show HOW A FEATURE affects predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is useful to answer questions like:\n",
    "\n",
    "- Controlling for all other house features, what impact do longitude and latitude have on home prices? To restate this, how would similarly sized houses be priced in different areas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like permutation importance, partial dependence plots are calculated after a model has been fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We take a row of data and we will use the fitted model to predict our outcome (probability their player won \"man of the match\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But we repeatedly alter the value for one variable to make a series of predictions (for instance, Ball Possession % is equal to 50 for that row, we make also predictions with other possible feature values: 20, 30, 60, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works\n",
    "\n",
    "\n",
    "- We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Interactions between features may cause the plot for a single row to be atypical. So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)\n",
    "tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)\n",
    "graphviz.Source(tree_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', module=\"matplotlib\")\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n",
    "pdp.pdp_plot(pdp_goals, 'Goal Scored')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n",
    "\n",
    "- A blue shaded area indicates level of confidence.\n",
    "\n",
    "- From this particular graph, we see that scoring a goal substantially increases your chances of winning \"Man of The Match.\" But extra goals beyond that appear to have little impact on predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature='Distance Covered (Kms)')\n",
    "pdp.pdp_plot(pdp_dist, 'Distance Covered (Kms)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2D Partial Dependence Plots\n",
    "\n",
    "\n",
    "- To see interactions between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Library bug fix:\n",
    "\n",
    "- https://github.com/SauceCat/PDPbox/commit/73c69665f1663b53984e187c7bc8996e25fea18e\n",
    "\n",
    "- Replace in pdp_plot_utils.py\n",
    "\n",
    "        251 inter_ax.clabel(c2, contour_label_fontsize=fontsize, inline=1)\n",
    "        with\n",
    "        251 inter_ax.clabel(c2, fontsize=fontsize, inline=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "features_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\n",
    "inter1 = pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n",
    "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km.\n",
    "\n",
    "- If they score 0 goals, distance covered doesn't matter.\n",
    "\n",
    "- But distance can impact predictions if they score goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But what if you want to break down how the model works for an individual prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- SHAP Values break down a prediction to show the impact of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where could you use this?\n",
    "\n",
    "- A model says a bank shouldn't loan someone money, and the bank is legally required to explain the basis for each loan rejection\n",
    "\n",
    "- A healthcare provider wants to identify what factors are driving each patient's risk of some disease so they can directly address those risk factors with targeted health interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- SHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.\n",
    "\n",
    "- Property: sum(SHAP values for all features) = prediction - pred_for_baseline_values\n",
    "\n",
    "    - That is, the SHAP values of all features sum up to explain why my prediction was different from the baseline.\n",
    "\n",
    "- Base value is the average model output (based on provided training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[row_to_show]\n",
    "rf_model.predict_proba(data_for_prediction.values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "\n",
    "# The shap_values object above is a list with two arrays. (in regression problems will be only one array)\n",
    "print(len(shap_values))\n",
    "\n",
    "# The first array is the SHAP values for a negative outcome (don't win the award),\n",
    "# and the second array is the list of SHAP values for the positive outcome (wins the award).\n",
    "print(len(shap_values[0]), len(shap_values[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SHAP Force Plot\n",
    "\n",
    "- To understand individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If you subtract the length of the blue bars from the length of the red bars, it equals the distance from the base value to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "About Explainers:\n",
    "\n",
    "- SHAP package has explainers for every type of model.\n",
    "\n",
    "- TreeExplainer works with Tree based models\n",
    "\n",
    "- DeepExplainer works with Deep Learning models\n",
    "\n",
    "- KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SHAP Summary Plot \n",
    "\n",
    "\n",
    "- It give us a birds-eye view of feature importance and what is driving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Calculating SHAP values can be slow.\n",
    "\n",
    "    - It isn't a problem here, because this dataset is small.\n",
    "    \n",
    "    - But you'll want to be careful when running these to plot with reasonably sized datasets.\n",
    "    \n",
    "    - The exception is when using an xgboost model, which SHAP has some optimizations for and which is thus much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(val_X)\n",
    "# We are considering the shap values for the prediction of \"True\" when we use shap_values[1]\n",
    "shap.summary_plot(shap_values[1], val_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Plot explanation\n",
    "\n",
    "\n",
    "- Each dot has three characteristics:\n",
    "\n",
    "    - Vertical location shows what feature it is depicting\n",
    "    - Color shows whether that feature was high or low for that row of the dataset\n",
    "    - Horizontal location shows whether the effect of that value caused a higher or lower prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Some things you should be able to easily pick out:\n",
    "\n",
    "    - The model ignored the Red and Yellow & Red features.\n",
    "\n",
    "    - Usually Yellow Card doesn't affect the prediction, but there is an extreme case where a high value caused a much lower prediction.\n",
    "\n",
    "    - High values of Goal scored caused higher predictions, and low values caused low predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SHAP Dependence Contribution Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Is the effect of having a certain value pretty constant, or does it vary a lot depending on the values of other features.\n",
    "\n",
    "- SHAP dependence contribution plots provide a similar insight to PDP's, but they add a lot more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X)\n",
    "shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The horizontal location is the actual value from the dataset.\n",
    "\n",
    "- The vertical location shows what having that value did to the prediction.\n",
    "\n",
    "- The fact this slopes upward says that the more you possess the ball, the higher the model's prediction is for winning the Man of the Match award."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")\n",
    "#interaction_index=\"auto\" is used to pick what seems to be the strongest interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot('Pass Accuracy %', shap_values[1], X, interaction_index=\"Ball Possession %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot('Pass Accuracy %', shap_values[1], X, interaction_index=\"Fouls Committed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you! :)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
